Time Cost Disect:

CPU->GPU:
1. 17.6s ----8GPUs/pipe
2. 14.4s (node0) / 29.3s (node1)      ----4GPUs/pipe, add 1 pipe/node
3. 7s (node0) / 14s (node1)    -----2GPUs/pipe, add 2 pipes/node

DDP rebuilt:
1. 5S ----- 8GPUs/pipe
2. 23s (node0) / 23s (node1)  --------4GPUs/pipe
3. 12s (node0) / 12s (node1) ------- 2GPUs/pipe



Optimization:

1. CPU pin_memory to GPU: reducing the CPU to GPU loading time cost:
Expermental Results: DOES NOT WORK

2. Avoid Rebuilt of DDP Bucket
Expermental Results: DOES NOT WORK


CUDA context memory cost:

(pipe_distributed) chaoyanghe@lambda-server1:~/PipeTransformer$ python
Python 3.7.4 (default, Aug 13 2019, 20:35:49)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> torch.zeros(2).to(0)
tensor([0., 0.], device='cuda:0')
>>>


867MiB