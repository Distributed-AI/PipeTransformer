
"""
  (module): PipeModelWrapper(
    (pipe_model): Pipe(
      (partitions): ModuleList(
        (0): Sequential(
          (0): Embeddings(
            (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (2): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (4): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (6): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): Sequential(
          (0): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (2): Sequential(
          (0): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): Sequential(
          (0): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (1): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (3): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): MultHeadAttentionLayer(
            (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (out): Linear(in_features=768, out_features=768, bias=True)
              (attn_dropout): Dropout(p=0.0, inplace=False)
              (proj_dropout): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
          )
          (5): MLPLayer(
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (ffn): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (7): OutputHead(
            (head): Linear(in_features=768, out_features=1000, bias=True)
          )
        )
      )
    )
  )
)

"""